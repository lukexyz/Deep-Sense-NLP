{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Document Classification\n",
    "* Bidirectional Encoding Representations from Transformers\n",
    "\n",
    "## Wikipedia Personal Attack / Offensive Language `Classification`\n",
    "\n",
    "The data set includes over 100k labeled discussion comments from English Wikipedia. Each comment was labeled by multiple annotators via Crowdflower on whether it contains a personal attack. There also contains demographic data for each crowd-worker.\n",
    "\n",
    "* `attack_annotated_comments.tsv` contains the comment text and some properties.\n",
    "* `attack_annotations.tsv` contains the labels (whether the comment contains an attack).\n",
    "\n",
    "\n",
    "Set up environment:\n",
    "\n",
    "___\n",
    "\n",
    "```\n",
    "> conda create -n BERT python=3.7 nb_conda_kernels pandas matplotlib seaborn\n",
    "> conda activate BERT\n",
    "> conda install tensorflow-gpu=1.15\n",
    "\n",
    "# for CUDA 10.2 ($nvcc --version)\n",
    "> pip install torch===1.4.0 torchvision===0.5.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "> pip install transformers scikit-learn keras tqdm ipywidgets\n",
    "> pip install -U jupyter\n",
    "```\n",
    "\n",
    "Continue with imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce RTX 2070\n",
      "Pytorch 1.4.0\n",
      "HuggingFace transformers 2.4.1\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "torch.cuda.device(0)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(f'Pytorch {torch.__version__}\\nHuggingFace transformers {transformers.__version__}')\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n",
      "tensorflow 1.15.0\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow GPU Check\n",
    "import tensorflow as tf\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')\n",
    "print(f'tensorflow {tf.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce RTX 2070\n"
     ]
    }
   ],
   "source": [
    "# Pytorch GPU Check\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Wikipedia Personal Attacks\n",
    "#### 1.1 Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not os.path.exists('./data/'): os.mkdir('./data/')\n",
    "\n",
    "files = [('./data/attack_annotated_comments.tsv', \n",
    "          'https://ndownloader.figshare.com/files/7554634'),\n",
    "         ('./data/attack_annotations.tsv', \n",
    "          'https://ndownloader.figshare.com/files/7554637')]    \n",
    "    \n",
    "for (filename, url) in files:\n",
    "    if not os.path.exists(filename):\n",
    "        print('Downloading', filename)\n",
    "        urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(115864, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rev_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37675</th>\n",
       "      <td>`-NEWLINE_TOKENThis is not ``creative``.  Thos...</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44816</th>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKEN:: the term ``stand...</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49851</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENTrue or false, the s...</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89320</th>\n",
       "      <td>Next, maybe you could work on being less cond...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93890</th>\n",
       "      <td>This page will need disambiguation.</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  comment  year  logged_in  \\\n",
       "rev_id                                                                       \n",
       "37675   `-NEWLINE_TOKENThis is not ``creative``.  Thos...  2002      False   \n",
       "44816   `NEWLINE_TOKENNEWLINE_TOKEN:: the term ``stand...  2002      False   \n",
       "49851   NEWLINE_TOKENNEWLINE_TOKENTrue or false, the s...  2002      False   \n",
       "89320    Next, maybe you could work on being less cond...  2002       True   \n",
       "93890                This page will need disambiguation.   2002       True   \n",
       "\n",
       "             ns  sample  split  \n",
       "rev_id                          \n",
       "37675   article  random  train  \n",
       "44816   article  random  train  \n",
       "49851   article  random  train  \n",
       "89320   article  random    dev  \n",
       "93890   article  random  train  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = pd.read_csv('./data/attack_annotated_comments.tsv', sep = '\\t', index_col = 0)\n",
    "annotations = pd.read_csv('./data/attack_annotations.tsv', sep = '\\t')\n",
    "\n",
    "print(comments.shape)\n",
    "comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset comes pre-split into `dev / test / train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dev</th>\n",
       "      <td>23160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>23178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>69526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       comment\n",
       "split         \n",
       "dev      23160\n",
       "test     23178\n",
       "train    69526"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments[['comment', 'split']].groupby('split').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>worker_id</th>\n",
       "      <th>quoting_attack</th>\n",
       "      <th>recipient_attack</th>\n",
       "      <th>third_party_attack</th>\n",
       "      <th>other_attack</th>\n",
       "      <th>attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37675</td>\n",
       "      <td>1362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37675</td>\n",
       "      <td>2408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37675</td>\n",
       "      <td>1493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37675</td>\n",
       "      <td>1439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37675</td>\n",
       "      <td>170</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rev_id  worker_id  quoting_attack  recipient_attack  third_party_attack  \\\n",
       "0   37675       1362             0.0               0.0                 0.0   \n",
       "1   37675       2408             0.0               0.0                 0.0   \n",
       "2   37675       1493             0.0               0.0                 0.0   \n",
       "3   37675       1439             0.0               0.0                 0.0   \n",
       "4   37675        170             0.0               0.0                 0.0   \n",
       "\n",
       "   other_attack  attack  \n",
       "0           0.0     0.0  \n",
       "1           0.0     0.0  \n",
       "2           0.0     0.0  \n",
       "3           0.0     0.0  \n",
       "4           0.0     0.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "      <th>attack</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rev_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>801279</th>\n",
       "      <td>Iraq is not good  ===  ===  USA is bad</td>\n",
       "      <td>2003</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2702703</th>\n",
       "      <td>____ fuck off you little asshole. If you wan...</td>\n",
       "      <td>2004</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4632658</th>\n",
       "      <td>i have a dick, its bigger than yours! hahaha</td>\n",
       "      <td>2004</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6545332</th>\n",
       "      <td>== renault ==  you sad little bpy for drivin...</td>\n",
       "      <td>2004</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6545351</th>\n",
       "      <td>== renault ==  you sad little bo for driving...</td>\n",
       "      <td>2004</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   comment  year  logged_in  \\\n",
       "rev_id                                                                        \n",
       "801279           Iraq is not good  ===  ===  USA is bad     2003      False   \n",
       "2702703    ____ fuck off you little asshole. If you wan...  2004      False   \n",
       "4632658       i have a dick, its bigger than yours! hahaha  2004      False   \n",
       "6545332    == renault ==  you sad little bpy for drivin...  2004       True   \n",
       "6545351    == renault ==  you sad little bo for driving...  2004       True   \n",
       "\n",
       "              ns   sample  split  attack  \n",
       "rev_id                                    \n",
       "801279   article   random  train    True  \n",
       "2702703     user   random  train    True  \n",
       "4632658  article  blocked  train    True  \n",
       "6545332     user  blocked  train    True  \n",
       "6545351     user  blocked   test    True  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label a comment as an atack if the majority of annoatators did so\n",
    "labels = annotations.groupby('rev_id')['attack'].mean() > 0.5\n",
    "\n",
    "# Join labels and comments\n",
    "comments['attack'] = labels\n",
    "\n",
    "# Remove newline and tab tokens\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "\n",
    "comments[comments.attack != False].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>attack</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dev</th>\n",
       "      <td>20405</td>\n",
       "      <td>2755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>20422</td>\n",
       "      <td>2756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>61447</td>\n",
       "      <td>8079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "attack  False  True \n",
       "split               \n",
       "dev     20405   2755\n",
       "test    20422   2756\n",
       "train   61447   8079"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target classes for the train-dev-test set\n",
    "comments.groupby(['split', 'attack']).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a simple text classifier\n",
    "train_comments = comments.query(\"split=='train'\")\n",
    "test_comments = comments.query(\"split=='test'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Examples of Obscene Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`  == DrK ==  Would this arrogant little shitkicker ``DrK`` stop reverting my\n",
      "changes to Nine Muses. He is too stupid to realize that this are not the Nine\n",
      "Muses of Greek Mythology` \n",
      "\n",
      "We dont cares about you,  is a fucking shit, bitch.............    : \n",
      "\n",
      "  ==Relevancy==    Are you from New York?  Are you well steeped in the history\n",
      "and contemporary culture of hardcore music? In my opinion, you should probably\n",
      "stop hanging around your computer all day waiting to make chagnes to Wikipedia\n",
      "articles.  Lighten up.  Get a hobby.      I politely request that you step off\n",
      "my balls. \n",
      "\n",
      " Has it occurred to you that I wouldn't tell someone to fuck off if they hadn't\n",
      "first referred to my edits as bullshit? And if NPA is not exhaustive, what\n",
      "criteria exactly are you using to decide that telling someone to fuck off counts\n",
      "as an attack? \n",
      "\n",
      "Wiki fags like NeilN (perfect screen name he's probably always on his knees) and\n",
      "CaliforniaKansan are devout Leftist propagandists intent on stifling speech and\n",
      "suppressing facts about their idols.  g \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import random\n",
    "\n",
    "# Wrap text to 80 characters.\n",
    "wrapper = textwrap.TextWrapper(width=80) \n",
    "\n",
    "# Filter to just the \"attack\" comments.\n",
    "attack_examples = train_comments.query('attack')['comment']\n",
    "\n",
    "# Randomly choose some examples.\n",
    "for i in range(5):\n",
    "    j = random.choice(attack_examples.index)\n",
    "    print(wrapper.fill(attack_examples[j]), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What percentage of comments are attacks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13,590 of 115,864 comments are attacks (11.73%)\n"
     ]
    }
   ],
   "source": [
    "total_comments = len(comments)\n",
    "num_attacks = len(comments.query('attack'))\n",
    "\n",
    "print(f'{num_attacks:,} of {total_comments:,} comments are attacks ({num_attacks/total_comments:.2%})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always predicting \"not attack\" will yield 88.11% accuracy on the test set.\n"
     ]
    }
   ],
   "source": [
    "prcnt_non_attack = 1 - (len(test_comments.query('attack')) / len(test_comments))\n",
    "print(f'Always predicting \"not attack\" will yield {prcnt_non_attack:.2%} accuracy on the test set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Transformer Limitation: Input Length\n",
    "\n",
    "\n",
    "* BERT has a maximum input length of 512 tokens. \n",
    "* Exploring how this limitation affects us in practice, and some possible approaches for addressing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first comment in the training set happens to be longer than 512 tokens, and doesn't contain an attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment 0 (not an attack) contains 591 WordPiece tokens.\n",
      "\n",
      "Original comment text:\n",
      "\n",
      "`- This is not ``creative``.  Those are the dictionary definitions of the terms\n",
      "``insurance`` and ``ensurance`` as properly applied to ``destruction``.  If you\n",
      "don't understand that, fine, legitimate criticism, I'll write up ``three man\n",
      "cell`` and ``bounty hunter`` and then it will be easy to understand why\n",
      "``ensured`` and ``insured`` are different - and why both differ from\n",
      "``assured``.  The sentence you quote is absolutely neutral.  You just aren't\n",
      "familiar with the underlying theory of strike-back (e.g. submarines as employed\n",
      "in nuclear warfare) guiding the insurance, nor likely the three man cell\n",
      "structure that kept the IRA from being broken by the British.  If that's my\n",
      "fault, fine, I can fix that to explain.  But ther'es nothing ``personal`` or\n",
      "``creative`` about it.  I'm tired of arguing with you.  Re: the other article,\n",
      "``multi-party`` turns up plenty, and there is more use of ``mutually`` than\n",
      "``mutual``.  If I were to apply your standard I'd be moving ``Mutual Assured\n",
      "Destruc\n"
     ]
    }
   ],
   "source": [
    "text = train_comments.iloc[0].comment\n",
    "\n",
    "# Run the tokenizer to count up the number of tokens. The tokenizer will split\n",
    "# the text into words, punctuation, and subwords as needed. \n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(f'Comment 0 (not an attack) contains {len(tokens):,} WordPiece tokens.')\n",
    "print('\\nOriginal comment text:\\n')\n",
    "print(wrapper.fill(text[:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now looking at how this text looks broken into tokens. We'll separate the first 512 tokens from the balance, in order to see how much text we'd lose by truncating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== First 512 tokens: ====\n",
      "\n",
      "` - this is not ` ` creative ` ` . those are the dictionary definitions of the\n",
      "terms ` ` insurance ` ` and ` ` en ##sur ##ance ` ` as properly applied to ` `\n",
      "destruction ` ` . if you don ' t understand that , fine , legitimate criticism ,\n",
      "i ' ll write up ` ` three man cell ` ` and ` ` bounty hunter ` ` and then it\n",
      "will be easy to understand why ` ` ensured ` ` and ` ` ins ##ured ` ` are\n",
      "different - and why both differ from ` ` assured ` ` . the sentence you quote is\n",
      "absolutely neutral . you just aren ' t familiar with the underlying theory of\n",
      "strike - back ( e . g . submarines as employed in nuclear warfare ) guiding the\n",
      "insurance , nor likely the three man cell structure that kept the ira from being\n",
      "broken by the british . if that ' s my fault , fine , i can fix that to explain\n",
      ". but the ##r ' es nothing ` ` personal ` ` or ` ` creative ` ` about it . i ' m\n",
      "tired of arguing with you . re : the other article , ` ` multi - party ` ` turns\n",
      "up plenty , and there is more use of ` ` mutually ` ` than ` ` mutual ` ` . if i\n",
      "were to apply your standard i ' d be moving ` ` mutual assured destruction ` `\n",
      "to ` ` talk ` ` for not appealing to a reagan voter ' s bias ##es about its\n",
      "effectiveness , and for dropping the ` ` l ##y ` ` . there is a double standard\n",
      "in your edit ##s . if it comes from some us history book , like ` ` peace\n",
      "movement ` ` or ' m . a . d . ' as defined in 1950 , you like it , even if the\n",
      "definition is totally useless in 2002 and only of historical interest . if it\n",
      "makes any even - obvious connection or implication from the language chosen in\n",
      "multiple profession - specific terms , you consider it somehow non - neutral . .\n",
      ". gandhi thinks ` ` eye for an eye ` ` describes riots , death penalty , and war\n",
      "all at once , but you don ' t . what do you know that gandhi doesn ' t ? guess\n",
      "what : reality is not neutral . current use of terms is slightly more\n",
      "controversial . neutrality requires negotiation , and some willingness to learn\n",
      ". this is your problem not mine . you may dislike the writing , fine , that can\n",
      "be fixed . but disregard ##ing fundamental ax ##ioms of phil ##os ##phy with\n",
      "names that rec ##ur in multiple phrases , or failing to make critical\n",
      "distinctions like ' insurance ' versus ' assurance ' versus ' en ##sur ##ance '\n",
      "( which\n",
      "\n",
      "==== Remaining 79 tokens: ====\n",
      "\n",
      "are made in one quote by an air force general in an in - context quote ) , is\n",
      "just a di ##sser ##vic ##e to the reader . if someone comes here to research a\n",
      "topic like mad , they want some context , beyond history . if this is a history\n",
      "book , fine , it ' s a history book . but that wasn ' t what it was claimed to\n",
      "be . . . `\n"
     ]
    }
   ],
   "source": [
    "# Print out the list of tokens\n",
    "print('==== First 512 tokens: ====\\n')\n",
    "print(wrapper.fill(str(' '.join(tokens[0:512]))))\n",
    "\n",
    "print('\\n==== Remaining {:,} tokens: ====\\n'.format(len(tokens) - 512))\n",
    "print(wrapper.fill(str(' '.join(tokens[512:]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Truncating observations to fit the 512 token limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment contains ~330 words.\n",
      "Comment contains ~20 sentences.\n"
     ]
    }
   ],
   "source": [
    "# First truncate the text to remove the last 79 tokens (which begin with the \n",
    "# words \"are made in\").\n",
    "last_char = text.find('are made in')\n",
    "\n",
    "# Truncate the text to only what fits in the 512 tokens.\n",
    "text = text[0:last_char]\n",
    "\n",
    "# Estimate the number of words in the comment by splitting it on whitespace.\n",
    "# First remove all double spaces.\n",
    "text = text.replace('  ', ' ')\n",
    "num_words = len(text.split(' '))\n",
    "\n",
    "print('Comment contains ~{:,} words.'.format(num_words))\n",
    "\n",
    "# Estimate the number of sentences by counting up the periods.\n",
    "num_sens = text.count('. ')\n",
    "\n",
    "print('Comment contains ~{:,} sentences.'.format(num_sens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Strategies for Longer Text\n",
    "\n",
    "Key points:\n",
    "* There's no *obvious* solution, but we'll cover a number of ideas to try.\n",
    "* Looking at the statistics of your dataset should help when choosing your strategy. We'll do this in section 3.3.\n",
    "\n",
    "There was a [paper published](https://arxiv.org/pdf/1905.05583.pdf) recently that investigated this problem and experimented with a few approaches.  Check out section 5.3. specifically. \n",
    "\n",
    "![BERT Text Classification Paper](http://www.mccormickml.com/assets/BERT/text_classification_paper.png)\n",
    "\n",
    "\n",
    "**Truncation**\n",
    "\n",
    "The simplest approach is just to drop some of the tokens, and hope that the remaining text is enough to perform the task well.\n",
    "\n",
    "You could drop tokens:\n",
    "* From the beginning of the text.\n",
    "* At the end of the text.\n",
    "* In the middle of the text.\n",
    "\n",
    "In the above paper, their experiments on the IMDb movie review dataset showed that keeping the first 128 tokens and the last 382 tokens performed best. (Note: This adds up to 510 tokens, leaving room for the special `[CLS]` and `[SEP]` tokens that we have to append to the beginning and the end of the text, respectively). \n",
    "\n",
    "Perhaps this is because, when writing something like a movie review, we tend to put our high-level points in the introduction and conclusion?\n",
    "\n",
    "**Chunking**\n",
    "\n",
    "Another approach attempted by the authors (and also proposed in [this thread](https://github.com/google-research/bert/issues/650) in the BERT GitHub repo) would be to divide the text into 512-token chunks and generate embeddings for these chunks separately. \n",
    "\n",
    "The authors of the paper combined the embeddings for the different chunks before performing the final classification. They tried several \"pooling\" strategies, such as averaging the embeddings together. None of these approaches outperformed the simple truncation approach, however.\n",
    "\n",
    "In the GitHub discussion above, the user suggested instead classifying the chunks separately and then averaging together the predictions. \n",
    "\n",
    "Mariano Kamp, in a comment on my YouTube video, also made the insightful observation that these \"chunking\" approaches could be computationly expensive when dealing with lengthy documents. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment Length Distribution\n",
    "To decide on a truncation strategy for this dataset, let's first look at the distribution of comment lengths.\n",
    "\n",
    "To do this, our first step is to tokenize all of the comments in the training set.\n",
    "\n",
    "**Tokenize All Comments**\n",
    "\n",
    "The `tokenizer.encode` function combines multiple steps for us:\n",
    "1. Split the sentence into tokens.\n",
    "2. Add the special `[CLS]` and `[SEP]` tokens.\n",
    "3. Map the tokens to their IDs.\n",
    "\n",
    "In order to explore the distribution of comment lengths, we will not perform any truncation here. Unfortunately, this results in the tokenizer spitting out a warning for every comment that's longer than 512 tokens. We'll just have to ignore those for now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (4.42.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Using cached ipywidgets-7.5.1-py2.py3-none-any.whl (121 kB)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from ipywidgets) (5.1.4)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from ipywidgets) (5.0.4)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from ipywidgets) (7.12.0)\n",
      "Collecting widgetsnbextension~=3.5.0\n",
      "  Using cached widgetsnbextension-3.5.1-py2.py3-none-any.whl (2.2 MB)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from ipywidgets) (4.3.3)\n",
      "Requirement already satisfied: tornado>=4.2 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (6.0.3)\n",
      "Requirement already satisfied: jupyter-client in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (5.3.4)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets) (3.2.0)\n",
      "Requirement already satisfied: ipython-genutils in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jupyter-core in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets) (4.6.1)\n",
      "Requirement already satisfied: jedi>=0.10 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.16.0)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.4.3)\n",
      "Requirement already satisfied: pygments in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (2.5.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (3.0.3)\n",
      "Requirement already satisfied: pickleshare in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (45.1.0.post20200119)\n",
      "Requirement already satisfied: decorator in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (4.4.1)\n",
      "Requirement already satisfied: backcall in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.1.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from widgetsnbextension~=3.5.0->ipywidgets) (6.0.3)\n",
      "Requirement already satisfied: six in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from traitlets>=4.3.1->ipywidgets) (1.14.0)\n",
      "Requirement already satisfied: pywin32>=1.0; sys_platform == \"win32\" in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (225)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.1)\n",
      "Requirement already satisfied: pyzmq>=13 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (18.1.1)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.14.11)\n",
      "Requirement already satisfied: attrs>=17.4.0 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (19.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: parso>=0.5.2 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from jedi>=0.10->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.6.1)\n",
      "Requirement already satisfied: wcwidth in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.1.8)\n",
      "Requirement already satisfied: jinja2 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.11.1)\n",
      "Requirement already satisfied: terminado>=0.8.1 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: prometheus-client in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: nbconvert in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (5.6.1)\n",
      "Requirement already satisfied: Send2Trash in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: testpath in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.4.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.4.2)\n",
      "Requirement already satisfied: defusedxml in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.6.0)\n",
      "Requirement already satisfied: bleach in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.1.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.3)\n",
      "Requirement already satisfied: webencodings in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n",
      "Installing collected packages: widgetsnbextension, ipywidgets\n",
      "Successfully installed ipywidgets-7.5.1 widgetsnbextension-3.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jupyter\n",
      "  Using cached jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
      "Requirement already satisfied, skipping upgrade: notebook in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from jupyter) (6.0.3)\n",
      "Collecting qtconsole\n",
      "  Using cached qtconsole-4.6.0-py2.py3-none-any.whl (121 kB)\n",
      "Requirement already satisfied, skipping upgrade: ipykernel in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from jupyter) (5.1.4)\n",
      "Requirement already satisfied, skipping upgrade: nbconvert in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from jupyter) (5.6.1)\n",
      "Collecting jupyter-console\n",
      "  Using cached jupyter_console-6.1.0-py2.py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied, skipping upgrade: ipywidgets in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from jupyter) (7.5.1)\n",
      "Requirement already satisfied, skipping upgrade: ipython-genutils in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from notebook->jupyter) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: jupyter-core>=4.6.1 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from notebook->jupyter) (4.6.1)\n",
      "Requirement already satisfied, skipping upgrade: terminado>=0.8.1 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from notebook->jupyter) (0.8.3)\n",
      "Requirement already satisfied, skipping upgrade: pyzmq>=17 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from notebook->jupyter) (18.1.1)\n",
      "Requirement already satisfied, skipping upgrade: nbformat in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from notebook->jupyter) (5.0.4)\n",
      "Requirement already satisfied, skipping upgrade: Send2Trash in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from notebook->jupyter) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: tornado>=5.0 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from notebook->jupyter) (6.0.3)\n",
      "Requirement already satisfied, skipping upgrade: jupyter-client>=5.3.4 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from notebook->jupyter) (5.3.4)\n",
      "Requirement already satisfied, skipping upgrade: prometheus-client in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from notebook->jupyter) (0.7.1)\n",
      "Requirement already satisfied, skipping upgrade: jinja2 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from notebook->jupyter) (2.11.1)\n",
      "Requirement already satisfied, skipping upgrade: traitlets>=4.2.1 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from notebook->jupyter) (4.3.3)\n",
      "Requirement already satisfied, skipping upgrade: pygments in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from qtconsole->jupyter) (2.5.2)\n",
      "Requirement already satisfied, skipping upgrade: ipython>=5.0.0 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from ipykernel->jupyter) (7.12.0)\n",
      "Requirement already satisfied, skipping upgrade: pandocfilters>=1.4.1 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from nbconvert->jupyter) (1.4.2)\n",
      "Requirement already satisfied, skipping upgrade: testpath in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from nbconvert->jupyter) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: defusedxml in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from nbconvert->jupyter) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: entrypoints>=0.2.2 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from nbconvert->jupyter) (0.3)\n",
      "Requirement already satisfied, skipping upgrade: mistune<2,>=0.8.1 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from nbconvert->jupyter) (0.8.4)\n",
      "Requirement already satisfied, skipping upgrade: bleach in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from nbconvert->jupyter) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from jupyter-console->jupyter) (3.0.3)\n",
      "Requirement already satisfied, skipping upgrade: widgetsnbextension~=3.5.0 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from ipywidgets->jupyter) (3.5.1)\n",
      "Requirement already satisfied, skipping upgrade: pywin32>=1.0; sys_platform == \"win32\" in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from jupyter-core>=4.6.1->notebook->jupyter) (225)\n",
      "Requirement already satisfied, skipping upgrade: jsonschema!=2.5.0,>=2.4 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from nbformat->notebook->jupyter) (3.2.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from jupyter-client>=5.3.4->notebook->jupyter) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from jinja2->notebook->jupyter) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: six in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from traitlets>=4.2.1->notebook->jupyter) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: decorator in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from traitlets>=4.2.1->notebook->jupyter) (4.4.1)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=18.5 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from ipython>=5.0.0->ipykernel->jupyter) (45.1.0.post20200119)\n",
      "Requirement already satisfied, skipping upgrade: backcall in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from ipython>=5.0.0->ipykernel->jupyter) (0.1.0)\n",
      "Requirement already satisfied, skipping upgrade: jedi>=0.10 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from ipython>=5.0.0->ipykernel->jupyter) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: pickleshare in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from ipython>=5.0.0->ipykernel->jupyter) (0.7.5)\n",
      "Requirement already satisfied, skipping upgrade: colorama; sys_platform == \"win32\" in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from ipython>=5.0.0->ipykernel->jupyter) (0.4.3)\n",
      "Requirement already satisfied, skipping upgrade: webencodings in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied, skipping upgrade: wcwidth in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter) (0.1.8)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook->jupyter) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook->jupyter) (19.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pyrsistent>=0.14.0 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook->jupyter) (0.14.11)\n",
      "Requirement already satisfied, skipping upgrade: parso>=0.5.2 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel->jupyter) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in d:\\programdata\\anaconda3\\envs\\bert\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema!=2.5.0,>=2.4->nbformat->notebook->jupyter) (2.1.0)\n",
      "Installing collected packages: qtconsole, jupyter-console, jupyter\n",
      "Successfully installed jupyter-1.0.0 jupyter-console-6.1.0 qtconsole-4.6.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing comments...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d870f29d8de242e7a298e79a87493f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=69526.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Read 0 comments.\n",
      "  Read 10,000 comments.\n",
      "  Read 20,000 comments.\n",
      "  Read 30,000 comments.\n",
      "  Read 40,000 comments.\n",
      "  Read 50,000 comments.\n",
      "  Read 60,000 comments.\n",
      "\n",
      "DONE.\n",
      "    69,526 comments\n",
      "Wall time: 3min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "# Record the length of each sequence (after truncating to 512).\n",
    "lengths = []\n",
    "\n",
    "print('Tokenizing comments...')\n",
    "\n",
    "# For every sentence...\n",
    "for sen in tqdm_notebook(train_comments.comment):\n",
    "    \n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sen,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 512,          # Truncate all sentences.                        \n",
    "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "    # Record the truncated length.\n",
    "    lengths.append(len(encoded_sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8,079 positive (contains attack)\n",
      " 61,447 negative (not an attack)\n"
     ]
    }
   ],
   "source": [
    "# Also retrieve the labels as a list.\n",
    "\n",
    "# Get the labels from the DataFrame, and convert from booleans to ints.\n",
    "labels = train_comments.attack.to_numpy().astype(int)\n",
    "\n",
    "print('{:>7,} positive (contains attack)'.format(np.sum(labels)))\n",
    "print('{:>7,} negative (not an attack)'.format(len(labels) - np.sum(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Min length: 2 tokens\n",
      "   Max length: 512 tokens\n",
      "Median length: 52.0 tokens\n"
     ]
    }
   ],
   "source": [
    "print('   Min length: {:,} tokens'.format(min(lengths)))\n",
    "print('   Max length: {:,} tokens'.format(max(lengths)))\n",
    "print('Median length: {:,} tokens'.format(np.median(lengths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the distribution of comment `length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, '# of Comments')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,3)\n",
    "\n",
    "# Truncate any comment lengths greater than 512.\n",
    "lengths = [min(l, 512) for l in lengths]\n",
    "\n",
    "# Plot the distribution of comment lengths.\n",
    "sns.distplot(lengths, kde=False, rug=False)\n",
    "\n",
    "# Alternatively, you might try using a log scale on the x-axis, but this is \n",
    "# tricky. See here for one approach:\n",
    "# https://stackoverflow.com/questions/47850202/plotting-a-histogram-on-a-log-scale-with-matplotlib?rq=1\n",
    "#plt.xscale('log')\n",
    "\n",
    "plt.title('Comment Lengths')\n",
    "plt.xlabel('Comment Length')\n",
    "plt.ylabel('# of Comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,668 of 69,526 sentences (2.4%) in the training set are longer than 512 tokens.\n"
     ]
    }
   ],
   "source": [
    "# Count the number of sentences that had to be truncated to 512 tokens.\n",
    "num_truncated = lengths.count(512)\n",
    "\n",
    "# Compare this to the total number of training sentences.\n",
    "num_sentences = len(lengths)\n",
    "prcnt = float(num_truncated) / float(num_sentences)\n",
    "\n",
    "print('{:,} of {:,} sentences ({:.1%}) in the training set are longer than 512 tokens.'.format(num_truncated, num_sentences, prcnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Of the truncated comments, how many contain a personal attack?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (0.0%) of the truncated examples contain a personal attack.\n"
     ]
    }
   ],
   "source": [
    "# Tally up how many of the truncated sentences are positive vs. negative examples.\n",
    "num_pos = 0\n",
    "num_neg = 0\n",
    "\n",
    "# Iterate through the comment lengths...\n",
    "for i, l in enumerate(lengths):\n",
    "    \n",
    "    # If the sentence was truncated...\n",
    "    if l == 512:\n",
    "\n",
    "        # Tally up whether it contains a personal attack or not.\n",
    "        if labels[l] == 1:\n",
    "            num_pos += 1\n",
    "        else:\n",
    "            num_neg += 1\n",
    "\n",
    "# Report the total.\n",
    "print('{:,} ({:.1%}) of the truncated examples contain a personal attack.'.format(num_pos, num_pos / (num_neg + num_pos)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense, as you wouldn't expect someone typing a hateful comment to spend a particularly long time on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad & Truncate the Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding/truncating all sentences to 128 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n"
     ]
    }
   ],
   "source": [
    "# We'll borrow the `pad_sequences` utility function to do this.\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Set the required sequence length.\n",
    "MAX_LEN = 128\n",
    "\n",
    "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "\n",
    "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "\n",
    "# Pad our input tokens with value 0.\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Masks\n",
    "The attention mask simply makes it explicit which tokens are actual words versus which are padding. \n",
    "\n",
    "The BERT vocabulary does not use the ID 0, so if a token ID is 0, then it's padding, and otherwise it's a real token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# For each sentence...\n",
    "for sent in input_ids:\n",
    "    \n",
    "    # Create the attention mask.\n",
    "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
    "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    \n",
    "    # Store the attention mask for this sentence.\n",
    "    attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for\n",
    "# training\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use 90% for training and 10% for validation.\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "# Do the same for the masks.\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
    "                                             random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all inputs and labels into torch tensors, the required datatype \n",
    "# for our model.\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose appropriate batch size depending on GPU memory availability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "\n",
    "# ============================== BATCH SIZE ================================ #\n",
    "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
    "# 16 or 32.\n",
    "\n",
    "# For RTX 2070 with 8 GB dedicated memory, bs=16 works best                  \n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "# ========================================================================== #\n",
    "\n",
    "\n",
    "# Create the DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set.\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II - BERT Fine-Tuning\n",
    "\n",
    "We'll be using [BertForSequenceClassification](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#bertforsequenceclassification) from the `HuggingFace` library. This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "            \"bert-base-uncased\",          # Use the 12-layer BERT model, with an uncased vocab.\n",
    "            num_labels = 2,               # The number of output labels--2 for binary classification.\n",
    "                                          # You can increase this for multi-class tasks.   \n",
    "            output_attentions = False,    # Whether the model returns attentions weights.\n",
    "            output_hidden_states = False) # Whether the model returns all hidden-states.\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model loaded we need to grab the training hyperparameters from within the stored model.\n",
    "\n",
    "For the purposes of fine-tuning, the authors recommend choosing from the following values:\n",
    "- Batch size: 16, 32  (We chose 32 when creating our DataLoaders).\n",
    "- Learning rate (Adam): 5e-5, 3e-5, 2e-5  (We'll use 2e-5).\n",
    "- Number of epochs: 2, 3, 4  (We'll use 4).\n",
    "\n",
    "The epsilon parameter `eps = 1e-8` is \"a very small number to prevent any division by zero in the implementation\" (from [here](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)).\n",
    "\n",
    "You can find the creation of the AdamW optimizer in `run_glue.py` [here](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L109)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5,    # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8)   # args.adam_epsilon  - default is 1e-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 1\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "loss_values = []\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 100 batches.\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        \n",
    "        \n",
    "        # =================================================================== #\n",
    "        \n",
    "        # dtype fix here change torch.cuda.IntTensor to scalar (type Long)\n",
    "        b_input_ids = batch[0].type(torch.LongTensor).to(device)\n",
    "        b_input_mask = batch[1].type(torch.LongTensor).to(device)\n",
    "        b_labels = batch[2].type(torch.LongTensor).to(device)\n",
    "        \n",
    "        # =================================================================== #\n",
    "\n",
    "\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # This will return the loss (rather than the model output) because we\n",
    "        # have provided the `labels`.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        \n",
    "        # The call to `model` always returns a tuple, so we need to pull the \n",
    "        # loss value out of the tuple.\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        \n",
    "        # =================================================================== #\n",
    "        # Fix for wrong dtype\n",
    "        \n",
    "        # b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        b_input_ids = batch[0].type(torch.LongTensor).to(device)\n",
    "        b_input_mask = batch[1].type(torch.LongTensor).to(device)\n",
    "        b_labels = batch[2].type(torch.LongTensor).to(device)\n",
    "        \n",
    "        # =================================================================== #\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have\n",
    "            # not provided labels.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at our training loss over all batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the learning curve.\n",
    "plt.rcParams[\"figure.figsize\"] = (6,4)\n",
    "plt.plot(loss_values, 'b-o')\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance On Test Set\n",
    "\n",
    "* Need to apply all of the same steps that we did for the training data to prepare our test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb088d235b1946ce96c88ebfb3e8aab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=23178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DONE.\n",
      "\n",
      "    23,178 test comments\n",
      "     2,756 positive (contains attack)\n",
      "    20,422 negative (not an attack)\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "test_input_ids = []\n",
    "\n",
    "# For every sentence...\n",
    "for sen in tqdm_notebook(test_comments.comment):\n",
    "    \n",
    "    # Report progress.\n",
    "    if ((len(input_ids) % 20000) == 0):\n",
    "        print('  Read {:,} comments.'.format(len(input_ids)))\n",
    "    \n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sen,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = MAX_LEN,          # Truncate all sentences.                        \n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.\n",
    "    test_input_ids.append(encoded_sent)\n",
    "\n",
    "print('Completed\\n')\n",
    "print('{:>10,} test comments'.format(len(test_input_ids)))\n",
    "\n",
    "# Also retrieve the labels as a list.\n",
    "\n",
    "# Get the labels from the DataFrame, and convert from booleans to ints.\n",
    "test_labels = test_comments.attack.to_numpy().astype(int)\n",
    "\n",
    "print('{:>10,} positive (contains attack)'.format(np.sum(test_labels)))\n",
    "print('{:>10,} negative (not an attack)'.format(len(test_labels) - np.sum(test_labels)))\n",
    "\n",
    "# Pad our input tokens\n",
    "test_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, \n",
    "                               dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Create attention masks\n",
    "test_attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in test_input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    test_attention_masks.append(seq_mask) \n",
    "\n",
    "# Convert to tensors.\n",
    "test_inputs = torch.tensor(test_input_ids)\n",
    "test_masks = torch.tensor(test_attention_masks)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "\n",
    "# Create the DataLoader.\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set Predictions\n",
    "With the test set prepared, we can apply our fine-tuned model to generate predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 23,178 test sentences...\n",
      "  Batch   100  of    725.    Elapsed: 0:00:17.\n",
      "  Batch   200  of    725.    Elapsed: 0:00:33.\n",
      "  Batch   300  of    725.    Elapsed: 0:00:50.\n",
      "  Batch   400  of    725.    Elapsed: 0:01:07.\n",
      "  Batch   500  of    725.    Elapsed: 0:01:24.\n",
      "  Batch   600  of    725.    Elapsed: 0:01:40.\n",
      "  Batch   700  of    725.    Elapsed: 0:01:57.\n",
      "    DONE.\n",
      "Wall time: 2min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Prediction on test set\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(test_inputs)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "predictions , true_labels = [], []\n",
    "t0 = time.time()\n",
    "\n",
    "# Predict \n",
    "for (step, batch) in enumerate(test_dataloader):\n",
    "    \n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "    # Progress update every 100 batches.\n",
    "    if step % 100 == 0 and not step == 0:\n",
    "        # Calculate elapsed time in minutes.\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "        \n",
    "        # Report progress.\n",
    "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    \n",
    "    # =================================================================== #\n",
    "    # Important dtype fix\n",
    "    \n",
    "    b_input_ids = batch[0].type(torch.LongTensor).to(device)\n",
    "    b_input_mask = batch[1].type(torch.LongTensor).to(device)\n",
    "    b_labels = batch[2].type(torch.LongTensor).to(device)\n",
    "    \n",
    "    # =================================================================== #\n",
    "  \n",
    "    # Telling the model not to compute or store gradients, saving memory and \n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        outputs = model(b_input_ids, token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the results across the batches.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_labels = np.concatenate(true_labels, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Predictions Dataframe\n",
    "\n",
    "`pred_0` describes confidence for `label 0` (no attack)  \n",
    "`pred_1` describes confidence for `label 1` (attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>split</th>\n",
       "      <th>attack</th>\n",
       "      <th>true_labels</th>\n",
       "      <th>pred_0</th>\n",
       "      <th>pred_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rev_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>155243</th>\n",
       "      <td>:If I may butt in  I've spent the last 1/4 h...</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>5.162868</td>\n",
       "      <td>-4.945552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177310</th>\n",
       "      <td>On my  you will find the apology that I owe...</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>5.358853</td>\n",
       "      <td>-4.972280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286174</th>\n",
       "      <td>Yep, that's Twin cities from which this ar...</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>5.344093</td>\n",
       "      <td>-4.993878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317177</th>\n",
       "      <td>See? I was right! ;-)</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>5.381514</td>\n",
       "      <td>-4.904042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386473</th>\n",
       "      <td>`    Thanks for fixing the spelling error in `...</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>5.388528</td>\n",
       "      <td>-4.971875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  comment split  attack  \\\n",
       "rev_id                                                                    \n",
       "155243    :If I may butt in  I've spent the last 1/4 h...  test   False   \n",
       "177310     On my  you will find the apology that I owe...  test   False   \n",
       "286174      Yep, that's Twin cities from which this ar...  test   False   \n",
       "317177                          See? I was right! ;-)      test   False   \n",
       "386473  `    Thanks for fixing the spelling error in `...  test   False   \n",
       "\n",
       "        true_labels    pred_0    pred_1  \n",
       "rev_id                                   \n",
       "155243            0  5.162868 -4.945552  \n",
       "177310            0  5.358853 -4.972280  \n",
       "286174            0  5.344093 -4.993878  \n",
       "317177            0  5.381514 -4.904042  \n",
       "386473            0  5.388528 -4.971875  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dx = test_comments[['comment', 'split', 'attack']].copy()\n",
    "dx['true_labels'] = true_labels\n",
    "dx['pred_0'] = predictions[:,0]\n",
    "dx['pred_1'] = predictions[:,1]\n",
    "dx.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Verification: `False Positives`\n",
    "\n",
    "Filtering test dataset for `non-attacks`, then sort observations by the highest `attack` prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>split</th>\n",
       "      <th>attack</th>\n",
       "      <th>true_labels</th>\n",
       "      <th>pred_0</th>\n",
       "      <th>pred_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rev_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>512598819</th>\n",
       "      <td>`  == Yo there! ==  You are a useless and fuck...</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.699845</td>\n",
       "      <td>4.373745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45024750</th>\n",
       "      <td>== Cause of Death ==  Fuck exicution, put do...</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.063657</td>\n",
       "      <td>3.941936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219083610</th>\n",
       "      <td>== Lick me ==  are u horny?</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.116485</td>\n",
       "      <td>3.932616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416308072</th>\n",
       "      <td>== IDIOT BYTCH ==  Once again, I only have 1...</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.060067</td>\n",
       "      <td>3.929109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74200889</th>\n",
       "      <td>==Ban this vandal== http://en.wikipedia.org/...</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.799661</td>\n",
       "      <td>3.624119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     comment split  attack  \\\n",
       "rev_id                                                                       \n",
       "512598819  `  == Yo there! ==  You are a useless and fuck...  test   False   \n",
       "45024750     == Cause of Death ==  Fuck exicution, put do...  test   False   \n",
       "219083610                        == Lick me ==  are u horny?  test   False   \n",
       "416308072    == IDIOT BYTCH ==  Once again, I only have 1...  test   False   \n",
       "74200889     ==Ban this vandal== http://en.wikipedia.org/...  test   False   \n",
       "\n",
       "           true_labels    pred_0    pred_1  \n",
       "rev_id                                      \n",
       "512598819            0 -4.699845  4.373745  \n",
       "45024750             0 -4.063657  3.941936  \n",
       "219083610            0 -4.116485  3.932616  \n",
       "416308072            0 -4.060067  3.929109  \n",
       "74200889             0 -3.799661  3.624119  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = dx[dx.attack == False].sort_values('pred_1', ascending=False)\n",
    "fp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positive: 0 \n",
      " `  == Yo there! ==  You are a useless and fucking douchebag. Go fuck yourself, you are a disgrace to wikipedia!!!!  YOU, SIR, ARE A BALLSACK!  You think you are so much smarter than everyone else!  Why do you think you are so smart!  I wish you would leave the Wikipedia!  I can't stand the way you B \n",
      "\n",
      "False Positive: 1 \n",
      "   == Cause of Death ==  Fuck exicution, put down Murder was the mutha fuckin' case, and Shawtnegger was tha murdera! \n",
      "\n",
      "False Positive: 2 \n",
      "   == Lick me ==  are u horny? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print('False Positive:', i, '\\n', fp.comment.values[i][:300], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reveals that a lot the false-positives are actually incorrectly labeled, and the model is predicting them as it is supposed to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Verification: `False Negatives`\n",
    "Text which is labelled as `attack`, but the model predicts as `safe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>split</th>\n",
       "      <th>attack</th>\n",
       "      <th>true_labels</th>\n",
       "      <th>pred_0</th>\n",
       "      <th>pred_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rev_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>687661161</th>\n",
       "      <td>`  ==Request to get 2015–16 Heart of Midlothia...</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>5.447719</td>\n",
       "      <td>-4.986903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67084641</th>\n",
       "      <td>== You only want what YOU feel is important ...</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>5.441694</td>\n",
       "      <td>-5.014874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459346236</th>\n",
       "      <td>`  == Unwarranted block ==  Please explain why...</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>5.424231</td>\n",
       "      <td>-4.975465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201181539</th>\n",
       "      <td>== Wow... ==  This page is so awful. It has ...</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>5.417169</td>\n",
       "      <td>-4.972826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67950017</th>\n",
       "      <td>Yeah, yeah you are definately narrow minded....</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>5.404196</td>\n",
       "      <td>-4.933738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     comment split  attack  \\\n",
       "rev_id                                                                       \n",
       "687661161  `  ==Request to get 2015–16 Heart of Midlothia...  test    True   \n",
       "67084641     == You only want what YOU feel is important ...  test    True   \n",
       "459346236  `  == Unwarranted block ==  Please explain why...  test    True   \n",
       "201181539    == Wow... ==  This page is so awful. It has ...  test    True   \n",
       "67950017     Yeah, yeah you are definately narrow minded....  test    True   \n",
       "\n",
       "           true_labels    pred_0    pred_1  \n",
       "rev_id                                      \n",
       "687661161            1  5.447719 -4.986903  \n",
       "67084641             1  5.441694 -5.014874  \n",
       "459346236            1  5.424231 -4.975465  \n",
       "201181539            1  5.417169 -4.972826  \n",
       "67950017             1  5.404196 -4.933738  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn = dx[dx.attack == True].sort_values('pred_0', ascending=False)\n",
    "fn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flase Negative: 0 \n",
      " `  ==Request to get 2015–16 Heart of Midlothian F.C. season &  deleted==  & . The article being well sourced is irrelevant. I added all those sources, I added all the content. I updated it every week. I am the main contributor and creator of the article. The point is people just aren't interested in updating these article. I will not be updating or editing at all any further. I can assure you that if you look at previous season article for the whole of Scottish football you will see the same pattern. It will be an outdated mess within weeks.   Does Wikipedia support having stale content. Does wikipedia refuse the right of people to have there own user pages deleted such as this . It is ridiculous to refuse this request. I fully intend to fight this in every way possible & I mean anyway possible. Wikipedia is a joke that just doesn't care about content, it cares about protecting itself and editors like .  ` \n",
      "\n",
      "Flase Negative: 1 \n",
      "   == You only want what YOU feel is important to be added?  You must be G.W.'s friend. ==  This editor does not feel that ALL the facts should be present in articles.  Does that sound like a smart editor?  (cur) (last)  2nd Piston Honda (Talk | contribs) (so ALL facts belong in the article?)  (cur) (last)  Playnine9 (Talk | contribs) m (it is a FACT, that's why it belongs. Explain to me why it DOESN'T belong.)   YES, ALL OF THE FACTS BELONG, YOU MORON!  Oh, and Hondas are slow. \n",
      "\n",
      "Flase Negative: 2 \n",
      " `  == Unwarranted block ==  Please explain why you blocked  for the reason ``Continued abuse after block expired``, when in fact only one edit was made after the block expired, which was a spelling correction.   ` \n",
      "\n",
      "Flase Negative: 3 \n",
      "   == Wow... ==  This page is so awful. It has some decent information but it looks like a 12 year old wrote a C grade essay. I'd like to get some people to help me fix it up.    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print('Flase Negative:', i, '\\n', fn.comment.values[i][:1000], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the difficult edge cases which the model incorrectly predicts as `non-attacks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy metric chosen by the authors for this dataset is the \"ROC AUC\" (Receiver Operating Characteristic, Area Under the Curve) rather than straight accuracy (number right / total examples).\n",
    "\n",
    "*ROC AUC*\n",
    "\n",
    "To illustrate the purpose of this metric, let's say that you were going to deploy this comment classifier on your website to automatically flag bad comments. In order to do that, you would have to make a decision about how confident you needed the classifier to be before flagging a comment as a personal attack.\n",
    "* If it was critical that no bad comments be missed, then you might choose to set a fairly low threshold, and then have a human review what the classifier flags. This would help ensure that bad comments would be caught, but at the cost of getting more false positives that the moderator would have to deal with.\n",
    "* If it wasn't critical to catch them all, and you wanted as few as possible to manually review, then you might you set a higher threshold so that you don't have as many flagged comments to review (at the risk of missing some attacks). \n",
    "\n",
    "The ROC AUC takes into account the fact that you can adjust the threshold to trade off false positives and false negatives, and yields a score which tries to capture overall accuracy independent of where you choose to put that threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC: 0.971\n",
      "Wall time: 63.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Our performance metric for the test set.\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Use the model output for label 1 as our predictions.\n",
    "p1 = predictions[:,1]\n",
    "\n",
    "# Calculate the ROC AUC.\n",
    "auc = roc_auc_score(true_labels, p1)\n",
    "\n",
    "print('Test ROC AUC: %.3f' %auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a great score!\n",
    "\n",
    "The baseline performance from the [original notebook](https://github.com/ewulczyn/wiki-detox/blob/master/src/figshare/Wikipedia%20Talk%20Data%20-%20Getting%20Started.ipynb) using tf-idf + Logistic Regression is 0.957.\n",
    "\n",
    "In a previous experiment (in my [word2vec course](https://www.chrismccormick.ai/word2vec-the-course)), I trained a word2vec model on this dataset and then used it as the embeddings for an LSTM classifier. That approach achieved a score of 0.966. \n",
    "\n",
    "We've beaten both with minimal effort using BERT!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first cell (taken from `run_glue.py` [here](https://github.com/huggingface/transformers/blob/35ff345fc9df9e777b27903f11fa213e4052595b/examples/run_glue.py#L495)) writes the model and tokenizer out to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./model_save/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model_save/vocab.txt',\n",
       " './model_save/special_tokens_map.json',\n",
       " './model_save/added_tokens.json')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "output_dir = './model_save/'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Good practice: save your training arguments together with the trained model\n",
    "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model From Saved State "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# The name of the folder containing the model files.\n",
    "output_dir = './model_save/'\n",
    "\n",
    "# Load our fine-tuned model, and configure it to return the \"hidden states\", \n",
    "# from which we will be taking our text embeddings.\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    output_dir,\n",
    "    output_hidden_states = True) # Whether the model returns all hidden-states.\n",
    "\n",
    "# Load the tokenizer.\n",
    "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "# Copy the model to the GPU.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to Embedding function\n",
    "\n",
    "Here we'll define a function which can take an arbitrary piece of text and use our fine-tuned BERT model to compute a feature vector (aka \"embedding\") for the text.\n",
    "\n",
    "Again, we'll be using the 768-feature vector corresponding to the special `[CLS]` token, taken from the final transformer layer.\n",
    "\n",
    "Note that, for converting large amounts of text, we could improve our performance by modifying this function to accept multiple input texts at once. GPUs (and CPUs) are most efficient when they are given a *batch* of inputs to work on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def text_to_embedding(tokenizer, model, in_text):\n",
    "    '''\n",
    "    Uses the provided BERT `model` and `tokenizer` to generate a vector \n",
    "    representation of the input string, `in_text`.\n",
    "\n",
    "    Returns the vector stored as a numpy ndarray.\n",
    "    '''\n",
    "\n",
    "    # ===========================\n",
    "    #    STEP 1: Tokenization\n",
    "    # ===========================\n",
    "\n",
    "    MAX_LEN = 128\n",
    "\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Truncate the sentence to MAX_LEN if necessary.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end. (After truncating!)\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    input_ids = tokenizer.encode(\n",
    "                        in_text,                    # Sentence to encode.\n",
    "                        add_special_tokens = True,  # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = MAX_LEN,       # Truncate all sentences.                        \n",
    "                   )    \n",
    "\n",
    "    # Pad our input tokens. Truncation was handled above by the `encode`\n",
    "    # function, which also makes sure that the `[SEP]` token is placed at the\n",
    "    # end *after* truncating.\n",
    "    # Note: `pad_sequences` expects a list of lists, but we only have one\n",
    "    # piece of text, so we surround `input_ids` with an extra set of brackets.\n",
    "    results = pad_sequences([input_ids], maxlen=MAX_LEN, dtype=\"long\", \n",
    "                              truncating=\"post\", padding=\"post\")\n",
    "    \n",
    "    # Remove the outer list.\n",
    "    input_ids = results[0]\n",
    "\n",
    "    # Create attention masks    \n",
    "    attn_mask = [int(i>0) for i in input_ids]\n",
    "    \n",
    "    \n",
    "    # Cast to tensors.\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attn_mask = torch.tensor(attn_mask)\n",
    "\n",
    "    # Add an extra dimension for the \"batch\" (even though there is only one \n",
    "    # input in this batch.)\n",
    "    input_ids = input_ids.unsqueeze(0)\n",
    "    attn_mask = attn_mask.unsqueeze(0)\n",
    "\n",
    "    # ===========================\n",
    "    #    STEP 2: BERT Model\n",
    "    # ===========================\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Copy the inputs to the GPU\n",
    "    \n",
    "    # Important include torch.LongTensor dtype fix\n",
    "    # =================================================================== #\n",
    "    input_ids = input_ids.type(torch.LongTensor).to(device)\n",
    "    attn_mask = attn_mask.type(torch.LongTensor).to(device)\n",
    "    # =================================================================== #\n",
    "    \n",
    "    # Telling the model not to build the backwards graph will make this \n",
    "    # a little quicker.\n",
    "    with torch.no_grad():        \n",
    "\n",
    "        # Forward pass, return hidden states and predictions.\n",
    "        # This will return the logits rather than the loss because we have\n",
    "        # not provided labels.\n",
    "        logits, encoded_layers = model(\n",
    "                                    input_ids = input_ids, \n",
    "                                    token_type_ids = None, \n",
    "                                    attention_mask = attn_mask)\n",
    "        \n",
    "    # Retrieve our sentence embedding--take the `[CLS]` embedding from the final\n",
    "    # layer.\n",
    "    layer_i = 12 # The last BERT layer before the classifier.\n",
    "    batch_i = 0 # Only one input in the batch.\n",
    "    token_i = 0 # The first token, corresponding to [CLS]\n",
    "        \n",
    "    # Grab the embedding.\n",
    "    vec = encoded_layers[layer_i][batch_i][token_i]\n",
    "\n",
    "    # Move to the CPU and convert to numpy ndarray.\n",
    "    vec = vec.detach().cpu().numpy()\n",
    "\n",
    "    return(vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Dataset\n",
    "\n",
    "We'll go back to the original dataset here. It should still be in memory as a pandas dataframe named `comments`.\n",
    "\n",
    "Let's start by vectorizing a single comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting embedding for sentence:\n",
      "\n",
      "       :Correct. Full biographical details will put down his birth details, etc.\n",
      "    It is just a marker to me at the moment to detail the WR aspect. He\n",
      "    certainly wasn't Belarus; as a geo-political entity it had no real existence\n",
      "    at the time. I have put a tbc marker on this article for now.\n",
      "\n",
      "Done. Embedding shape: (768,)\n"
     ]
    }
   ],
   "source": [
    "# Get the text from one of the comments.\n",
    "input_text = comments.iloc[10].comment\n",
    "\n",
    "# Use `textwrap` to print the sentence nicely.\n",
    "wrapper = textwrap.TextWrapper(initial_indent=\"    \", subsequent_indent=\"    \", \n",
    "                               width = 80)\n",
    "\n",
    "print('Getting embedding for sentence:\\n\\n', wrapper.fill(input_text))\n",
    "\n",
    "# Use the BERT model and tokenizer to generate an embedding for `input_text`.\n",
    "vec = text_to_embedding(tokenizer, model, input_text)\n",
    "\n",
    "print('\\nDone. Embedding shape:', str(vec.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform similarity search against this comments dataset, we now need to vectorize *all* of the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over all of the comments in the dataset, converting each one. Prints progress periodically. This took just under 1 hour for me to run in Colab with a T4 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sentence embeddings for all 115,864 comments...\n",
      "  Comment   2,000  of  115,864.    Elapsed: 0:02:19. Remaining: 2:11:40\n",
      "  Comment   4,000  of  115,864.    Elapsed: 0:04:33. Remaining: 2:07:08\n",
      "  Comment   6,000  of  115,864.    Elapsed: 0:06:58. Remaining: 2:07:35\n",
      "  Comment   8,000  of  115,864.    Elapsed: 0:09:09. Remaining: 2:03:21\n",
      "  Comment  10,000  of  115,864.    Elapsed: 0:11:21. Remaining: 2:00:08\n",
      "  Comment  12,000  of  115,864.    Elapsed: 0:13:48. Remaining: 1:59:31\n",
      "  Comment  14,000  of  115,864.    Elapsed: 0:16:10. Remaining: 1:57:40\n",
      "  Comment  16,000  of  115,864.    Elapsed: 0:18:21. Remaining: 1:54:34\n",
      "  Comment  18,000  of  115,864.    Elapsed: 0:20:33. Remaining: 1:51:45\n",
      "  Comment  20,000  of  115,864.    Elapsed: 0:22:44. Remaining: 1:48:57\n",
      "  Comment  22,000  of  115,864.    Elapsed: 0:24:54. Remaining: 1:46:13\n",
      "  Comment  24,000  of  115,864.    Elapsed: 0:27:16. Remaining: 1:44:21\n",
      "  Comment  26,000  of  115,864.    Elapsed: 0:29:29. Remaining: 1:41:56\n",
      "  Comment  28,000  of  115,864.    Elapsed: 0:31:44. Remaining: 1:39:36\n",
      "  Comment  30,000  of  115,864.    Elapsed: 0:34:02. Remaining: 1:37:23\n",
      "  Comment  32,000  of  115,864.    Elapsed: 0:36:21. Remaining: 1:35:15\n",
      "  Comment  34,000  of  115,864.    Elapsed: 0:38:42. Remaining: 1:33:10\n",
      "  Comment  36,000  of  115,864.    Elapsed: 0:40:54. Remaining: 1:30:45\n",
      "  Comment  38,000  of  115,864.    Elapsed: 0:43:13. Remaining: 1:28:34\n",
      "  Comment  40,000  of  115,864.    Elapsed: 0:45:36. Remaining: 1:26:29\n",
      "  Comment  42,000  of  115,864.    Elapsed: 0:48:02. Remaining: 1:24:29\n",
      "  Comment  44,000  of  115,864.    Elapsed: 0:50:19. Remaining: 1:22:10\n",
      "  Comment  46,000  of  115,864.    Elapsed: 0:52:33. Remaining: 1:19:49\n",
      "  Comment  48,000  of  115,864.    Elapsed: 0:54:38. Remaining: 1:17:15\n",
      "  Comment  50,000  of  115,864.    Elapsed: 0:56:45. Remaining: 1:14:45\n",
      "  Comment  52,000  of  115,864.    Elapsed: 0:58:54. Remaining: 1:12:20\n",
      "  Comment  54,000  of  115,864.    Elapsed: 1:01:04. Remaining: 1:09:57\n",
      "  Comment  56,000  of  115,864.    Elapsed: 1:03:13. Remaining: 1:07:34\n",
      "  Comment  58,000  of  115,864.    Elapsed: 1:05:22. Remaining: 1:05:13\n",
      "  Comment  60,000  of  115,864.    Elapsed: 1:07:26. Remaining: 1:02:47\n",
      "  Comment  62,000  of  115,864.    Elapsed: 1:09:30. Remaining: 1:00:23\n",
      "  Comment  64,000  of  115,864.    Elapsed: 1:11:36. Remaining: 0:58:01\n",
      "  Comment  66,000  of  115,864.    Elapsed: 1:13:50. Remaining: 0:55:47\n",
      "  Comment  68,000  of  115,864.    Elapsed: 1:16:06. Remaining: 0:53:34\n",
      "  Comment  70,000  of  115,864.    Elapsed: 1:18:25. Remaining: 0:51:23\n",
      "  Comment  72,000  of  115,864.    Elapsed: 1:20:44. Remaining: 0:49:11\n",
      "  Comment  74,000  of  115,864.    Elapsed: 1:22:58. Remaining: 0:46:56\n",
      "  Comment  76,000  of  115,864.    Elapsed: 1:25:04. Remaining: 0:44:37\n",
      "  Comment  78,000  of  115,864.    Elapsed: 1:27:12. Remaining: 0:42:20\n",
      "  Comment  80,000  of  115,864.    Elapsed: 1:29:16. Remaining: 0:40:01\n",
      "  Comment  82,000  of  115,864.    Elapsed: 1:31:22. Remaining: 0:37:44\n",
      "  Comment  84,000  of  115,864.    Elapsed: 1:33:33. Remaining: 0:35:29\n",
      "  Comment  86,000  of  115,864.    Elapsed: 1:35:50. Remaining: 0:33:17\n",
      "  Comment  88,000  of  115,864.    Elapsed: 1:38:06. Remaining: 0:31:04\n",
      "  Comment  90,000  of  115,864.    Elapsed: 1:40:16. Remaining: 0:28:49\n",
      "  Comment  92,000  of  115,864.    Elapsed: 1:42:28. Remaining: 0:26:35\n",
      "  Comment  94,000  of  115,864.    Elapsed: 1:44:34. Remaining: 0:24:19\n",
      "  Comment  96,000  of  115,864.    Elapsed: 1:46:38. Remaining: 0:22:04\n",
      "  Comment  98,000  of  115,864.    Elapsed: 1:48:46. Remaining: 0:19:50\n",
      "  Comment 100,000  of  115,864.    Elapsed: 1:50:56. Remaining: 0:17:36\n",
      "  Comment 102,000  of  115,864.    Elapsed: 1:53:10. Remaining: 0:15:23\n",
      "  Comment 104,000  of  115,864.    Elapsed: 1:55:23. Remaining: 0:13:10\n",
      "  Comment 106,000  of  115,864.    Elapsed: 1:57:34. Remaining: 0:10:56\n",
      "  Comment 108,000  of  115,864.    Elapsed: 1:59:40. Remaining: 0:08:43\n",
      "  Comment 110,000  of  115,864.    Elapsed: 2:01:57. Remaining: 0:06:30\n",
      "  Comment 112,000  of  115,864.    Elapsed: 2:04:19. Remaining: 0:04:17\n",
      "  Comment 114,000  of  115,864.    Elapsed: 2:06:41. Remaining: 0:02:04\n",
      "Wall time: 2h 8min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t0 = time.time()\n",
    "\n",
    "# Store the set of embeddings.\n",
    "embeddings = []\n",
    "num_comments = len(comments)\n",
    "print('Generating sentence embeddings for all {:,} comments...'.format(num_comments))\n",
    "\n",
    "row_num = 0\n",
    "\n",
    "# For each row of the dataframe...\n",
    "for index, row in comments.iterrows():\n",
    "\n",
    "    # Progress update every 2,000 comments.\n",
    "    if row_num % 2000 == 0 and not row_num == 0:\n",
    "\n",
    "        # Calculate elapsed time and format it.\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "        \n",
    "        # Calculate the time remaining based on our progress.\n",
    "        rows_per_sec = (time.time() - t0) / row_num\n",
    "        remaining_sec = rows_per_sec * (num_comments - row_num)\n",
    "        remaining = format_time(remaining_sec)\n",
    "\n",
    "        # Report progress.\n",
    "        print('  Comment {:>7,}  of  {:>7,}.    Elapsed: {:}. Remaining: {:}'.format(row_num, num_comments, elapsed, remaining))\n",
    "\n",
    "    # Vectorize this comment.\n",
    "    vec = text_to_embedding(tokenizer, model, row.comment)\n",
    "\n",
    "    # Store the embeddings.\n",
    "    embeddings.append(vec) \n",
    "\n",
    "    row_num += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since creating those embeddings was such a lengthy process, let's be sure to save the embeddings to disk in case we want to reload them another time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector shape: (115864, 768)\n",
      "Saving embeddings to: ./model_save/embeddings.npy\n"
     ]
    }
   ],
   "source": [
    "# Convert the list of vectors into a 2D array.\n",
    "vecs = np.stack(embeddings)\n",
    "print('vector shape:', vecs.shape)\n",
    "\n",
    "output_dir = './model_save/'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Use numpy to write out the matrix of embeddings.\n",
    "print(\"Saving embeddings to: ./model_save/embeddings.npy\")\n",
    "np.save('./model_save/embeddings.npy', vecs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Similarity Search\n",
    "\n",
    "Now that we have our comments all vectorized, we are ready to make them \"searchable\".\n",
    "\n",
    "We do this using a technique called \"k-Nearest Neighbor Search\" or \"k-NN\". Simply put, we use a distance metric such as Euclidean distance, calculate that distance between our \"query\" vector and all of the vectors to be searched, then sort the distances to find the closest matches.\n",
    "\n",
    "All of those distance calculations can make k-NN search computationally expensive and slow. There are a number of libraries out there for accelerating k-NN using carefully optimized code and / or approximation techniques.\n",
    "\n",
    "I personally like the FAISS (Facebook AI Similarity Search) library, in part because it has a really excellent GPU implementation.\n",
    "\n",
    "Using the GPU, we can perform \"brute-force\" k-NN search (meaning no approximation techniques which compromise on accuracy) on this dataset quickly.\n",
    "\n",
    "### 8.1. k-NN with FAISS and GPU\n",
    "\n",
    "To create a GPU index with FAISS, you first create it on the CPU, then copy it over.\n",
    "\n",
    "Note: Following is for unix only (faiss-cpu is not supported on windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import faiss\n",
    "\n",
    "# =====================================\n",
    "#            FAISS Setup\n",
    "# =====================================\n",
    "\n",
    "# Build a flat (CPU) index\n",
    "cpu_index = faiss.IndexFlatL2(vecs.shape[1])\n",
    "\n",
    "# Use 1 GPU.\n",
    "n_gpu = 1\n",
    "\n",
    "# Print the number of available GPUs. \n",
    "print('Number of available GPUs: %d    Using: %d' % (faiss.get_num_gpus(), n_gpu))\n",
    "\n",
    "# If using multiple GPUs, enable sharding so that the dataset is divided across \n",
    "# the GPUs rather than replicated.\n",
    "co = faiss.GpuMultipleClonerOptions()\n",
    "co.shard = True\n",
    "\n",
    "# Make it into a gpu index\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index, co=co, ngpu=n_gpu)\n",
    "\n",
    "# Add vecs to our GPU index\n",
    "print('Adding dataset to index...')\n",
    "t0 = time.time()    \n",
    "\n",
    "gpu_index.add(vecs)\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print('Building index took %.2f seconds' % (elapsed))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try taking one of the comments from the dataset, and searching for the most semantically similar comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment number 4 is short and sweet.\n",
    "print('==== Input Comment =====')\n",
    "print('Comment #4:')\n",
    "print(wrapper.fill(comments.iloc[4].comment))\n",
    "\n",
    "# Let's find the 5 most similar comments.\n",
    "D, I = gpu_index.search(vecs[4].reshape(1, 768), k=5) \n",
    "\n",
    "print('')\n",
    "print('==== Top 5 Results ====')\n",
    "\n",
    "# For each result...\n",
    "for i in range(I.shape[1]):\n",
    "\n",
    "    # Look up the comment row number for this result.\n",
    "    result_i = I[0, i]\n",
    "\n",
    "    # Look up the text for this comment.\n",
    "    text = comments.iloc[result_i].comment\n",
    "\n",
    "    print('Comment #{:,}:'.format(result_i))\n",
    "    print('L2 Distance: %.2f' % D[0, i])\n",
    "    print(wrapper.fill('\"' + text + '\"'))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our `text_to_embedding` function, we can also define new text to use as our query. Let's try writing a new sentence that's similar in meaning to comment #4, but uses different language. The word \"disambiguate\" means \"remove uncertainty of meaning from\", so I've written \"The meaning of this page needs to be clarified.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"The meaning of this page needs to be clarified.\"\n",
    "\n",
    "# Vectorize a new piece of text.\n",
    "query_vec = text_to_embedding(tokenizer, model, query_text)\n",
    "\n",
    "# Let's find the 5 most similar comments.\n",
    "D, I = gpu_index.search(query_vec.reshape(1, 768), k=5) \n",
    "\n",
    "print('')\n",
    "print('==== Top 5 Results ====')\n",
    "\n",
    "# For each result...\n",
    "for i in range(I.shape[1]):\n",
    "\n",
    "    # Look up the comment row number for this result.\n",
    "    result_i = I[0, i]\n",
    "\n",
    "    # Look up the text for this comment.\n",
    "    text = comments.iloc[result_i].comment\n",
    "\n",
    "    print('Comment #{:,}:'.format(result_i))\n",
    "    print('L2 Distance: %.2f' % D[0, i])\n",
    "    print(wrapper.fill('\"' + text + '\"'))\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"This article really needs more citations.\"\n",
    "\n",
    "# Vectorize a new piece of text.\n",
    "query_vec = text_to_embedding(tokenizer, model, query_text)\n",
    "\n",
    "# Let's find the 5 most similar comments.\n",
    "D, I = gpu_index.search(query_vec.reshape(1, 768), k=5) \n",
    "\n",
    "print('')\n",
    "print('==== Top 5 Results ====')\n",
    "\n",
    "# For each result...\n",
    "for i in range(I.shape[1]):\n",
    "\n",
    "    # Look up the comment row number for this result.\n",
    "    result_i = I[0, i]\n",
    "\n",
    "    # Look up the text for this comment.\n",
    "    text = comments.iloc[result_i].comment\n",
    "\n",
    "    print('Comment #{:,}:'.format(result_i))\n",
    "    print('L2 Distance: %.2f' % D[0, i])\n",
    "    print(wrapper.fill('\"' + text + '\"'))\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acknowledgements:\n",
    "* Chris McCormick  \n",
    "`BERT Document Classification` https://www.youtube.com/watch?v=_eSGWNqKeeY"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:BERT] *",
   "language": "python",
   "name": "conda-env-BERT-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
